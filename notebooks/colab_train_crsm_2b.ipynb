{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5be3583",
   "metadata": {},
   "source": [
    "# Training a 2B Parameter CRSM Model on Google Colab\n",
    "\n",
    "This notebook provides a proof-of-concept implementation for training a ~2B parameter language model using the CRSM architecture. It includes:\n",
    "- Base model pretraining\n",
    "- Instruction tuning\n",
    "- Model validation and evaluation\n",
    "- Colab-specific optimizations\n",
    "\n",
    "Make sure to:\n",
    "1. Use a GPU runtime (preferably A100)\n",
    "2. Mount Google Drive for model checkpoints\n",
    "3. Set batch sizes according to available memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec94d9",
   "metadata": {},
   "source": [
    "# 1. Environment and GPU Check\n",
    "\n",
    "First, let's verify we have the correct runtime and resources available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96274f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import subprocess\n",
    "from pynvml import *\n",
    "\n",
    "def check_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"No GPU available. Please select GPU runtime in Colab.\")\n",
    "    \n",
    "    # Get GPU info\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    gpu_model = nvmlDeviceGetName(handle).decode('utf-8')\n",
    "    gpu_memory_gb = info.total / (1024**3)\n",
    "    \n",
    "    print(f\"GPU: {gpu_model}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.1f}GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Get system memory\n",
    "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    print(f\"System RAM: {ram_gb:.1f}GB\")\n",
    "    \n",
    "    # Determine safe batch size based on GPU\n",
    "    if 'A100' in gpu_model:\n",
    "        return {'batch_size': 32, 'gradient_accum': 4}\n",
    "    elif gpu_memory_gb > 24:  # V100 or similar\n",
    "        return {'batch_size': 16, 'gradient_accum': 8}\n",
    "    else:  # T4 or similar\n",
    "        return {'batch_size': 8, 'gradient_accum': 16}\n",
    "\n",
    "training_params = check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061dc6db",
   "metadata": {},
   "source": [
    "# 2. Install Required Packages\n",
    "\n",
    "Let's install the necessary packages for training our CRSM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c509ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate bitsandbytes evaluate huggingface-hub sentencepiece\n",
    "!pip install -q pytorch-lightning wandb\n",
    "\n",
    "# Install CRSM from the current directory\n",
    "!git clone https://github.com/pomilon/CRSM.git\n",
    "%cd CRSM\n",
    "!pip install -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abc3c",
   "metadata": {},
   "source": [
    "# 3. Mount Google Drive and Set Up Workspace\n",
    "\n",
    "We'll mount Google Drive to store our model checkpoints and set up our workspace directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ef0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up workspace directories\n",
    "WORKSPACE_DIR = \"/content/drive/MyDrive/crsm_2b\"\n",
    "CHECKPOINTS_DIR = f\"{WORKSPACE_DIR}/checkpoints\"\n",
    "DATA_DIR = f\"{WORKSPACE_DIR}/data\"\n",
    "LOGS_DIR = f\"{WORKSPACE_DIR}/logs\"\n",
    "\n",
    "for d in [WORKSPACE_DIR, CHECKPOINTS_DIR, DATA_DIR, LOGS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Workspace directories created at:\", WORKSPACE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba12965",
   "metadata": {},
   "source": [
    "# 4. Download and Prepare Datasets\n",
    "\n",
    "We'll use a mix of:\n",
    "- RedPajama for base model training (subset)\n",
    "- Alpaca/Dolly for instruction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ed2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load a subset of RedPajama for base training\n",
    "base_dataset = load_dataset(\"cerebras/SlimPajama-627B\", \n",
    "                          streaming=True,\n",
    "                          split=\"train\")\n",
    "\n",
    "# Filter and prepare base dataset\n",
    "def prepare_base_data():\n",
    "    data = []\n",
    "    for item in base_dataset.take(10000):  # Limit for PoC\n",
    "        if len(item['text']) > 100:  # Filter very short texts\n",
    "            data.append(item['text'])\n",
    "    \n",
    "    # Split train/val\n",
    "    random.shuffle(data)\n",
    "    split = int(len(data) * 0.9)\n",
    "    return {\n",
    "        'train': data[:split],\n",
    "        'validation': data[split:]\n",
    "    }\n",
    "\n",
    "# Load instruction dataset\n",
    "instruct_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n",
    "messages_by_id = {msg['message_id']: msg for msg in instruct_dataset}\n",
    "\n",
    "# Prepare instruction data\n",
    "def prepare_instruct_data():\n",
    "    data = []\n",
    "    for item in instruct_dataset:\n",
    "        # Only process assistant responses that have a parent (human prompt)\n",
    "        if item['role'] == 'assistant' and item['parent_id'] in messages_by_id:\n",
    "            human_prompt = messages_by_id[item['parent_id']]['text']\n",
    "            assistant_response = item['text']\n",
    "            \n",
    "            # Format as instruction tuple\n",
    "            data.append({\n",
    "                'instruction': human_prompt,\n",
    "                'input': '',  # OpenAssistant format doesn't separate input\n",
    "                'output': assistant_response\n",
    "            })\n",
    "    \n",
    "    # Split train/val\n",
    "    random.shuffle(data)\n",
    "    split = int(len(data) * 0.9)\n",
    "    return {\n",
    "        'train': data[:split],\n",
    "        'validation': data[split:]\n",
    "    }\n",
    "\n",
    "# Save processed datasets\n",
    "base_data = prepare_base_data()\n",
    "instruct_data = prepare_instruct_data()\n",
    "\n",
    "print(f\"Base dataset size: {len(base_data['train'])} train, {len(base_data['validation'])} validation\")\n",
    "print(f\"Instruction dataset size: {len(instruct_data['train'])} train, {len(instruct_data['validation'])} validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62244cf3",
   "metadata": {},
   "source": [
    "# 5. Initialize CRSM Model and Tokenizer\n",
    "\n",
    "Let's configure and initialize our CRSM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import crsm\n",
    "from crsm.model import CRSMConfig, CRSMModel\n",
    "from crsm.tokenizer import Tokenizer\n",
    "\n",
    "# Model configuration for ~2B parameters\n",
    "config = CRSMConfig(\n",
    "    vocab_size=32000,  # Will be updated after tokenizer setup\n",
    "    hidden_size=2048,\n",
    "    intermediate_size=8192,\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=2048,\n",
    "    d_state=256,  # Increased for larger model\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Initialize tokenizer (uses HF AutoTokenizer if a name is provided to Tokenizer, otherwise uses SimpleVocab fallback)\n",
    "# For Colab PoC we use the simple fallback unless you provide a pretrained tokenizer name below.\n",
    "HF_TOKENIZER_NAME = None  # set to e.g. 'gpt2' to use a standard HF tokenizer\n",
    "if HF_TOKENIZER_NAME:\n",
    "    tokenizer = Tokenizer(hf_name=HF_TOKENIZER_NAME)\n",
    "else:\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "# Save tokenizer in a compatible way\n",
    "tokenizer_dir = f\"{WORKSPACE_DIR}/tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "if getattr(tokenizer, \"_hf\", None) is not None:\n",
    "    tokenizer._hf.save_pretrained(tokenizer_dir)\n",
    "else:\n",
    "    # Save simple vocab mapping\n",
    "    vocab = {\"itos\": tokenizer._simple.itos, \"stoi\": tokenizer._simple.stoi}\n",
    "    with open(os.path.join(tokenizer_dir, \"simple_vocab.json\"), \"w\") as f:\n",
    "        json.dump(vocab, f)\n",
    "\n",
    "# Update config with actual vocab size\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Initialize model\n",
    "model = CRSMModel(config)\n",
    "\n",
    "# Move model to GPU if available (explicitly place weights on CUDA VRAM)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# When using mixed precision training Lightning will handle autocast/optimizer states,\n",
    "# but explicitly set model to half if you want to pre-convert weights to fp16 for memory savings\n",
    "# (only do this if you understand mixed-precision implications):\n",
    "# if device.type == 'cuda':\n",
    "#     model.half()\n",
    "\n",
    "# Print model size\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters()) / 1e9\n",
    "\n",
    "print(f\"Model size: {count_parameters(model):.2f}B parameters\")\n",
    "\n",
    "# Note: Lightning Trainer will move the LightningModule to the correct device as well.\n",
    "# If you wrap `model` into a LightningModule before Trainer.fit, Lightning will relocate as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d377e70",
   "metadata": {},
   "source": [
    "# 6. Set Up Training Pipeline\n",
    "\n",
    "Now we'll set up the training pipeline with data loading, optimization, and logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d92e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from crsm.dataset import StreamingTextDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CRSMLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, learning_rate=3e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Expect model to return (logits, states) - loss computation must be implemented in training loop or model\n",
    "        logits, _ = self.model(batch['input_ids'].to(self.device))\n",
    "        # Simple cross-entropy loss over next tokens\n",
    "        shift_logits = logits[:, :-1, :]\n",
    "        shift_labels = batch['labels'].to(self.device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits, _ = self.model(batch['input_ids'].to(self.device))\n",
    "        shift_logits = logits[:, :-1, :]\n",
    "        shift_labels = batch['labels'].to(self.device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=1000)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "# In-memory dataset for lists of raw text\n",
    "class InMemoryTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=128):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        ids = []\n",
    "        for t in texts:\n",
    "            toks = tokenizer.encode(t)\n",
    "            ids.extend(toks)\n",
    "        self.ids = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.ids) < self.seq_len:\n",
    "            return 0\n",
    "        return len(self.ids) - self.seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.ids[idx: idx + self.seq_len]\n",
    "        input_ids = seq[:-1].clone()\n",
    "        labels = seq[1:].clone()\n",
    "        # Ensure labels are in valid range: [0, vocab_size) or -100 for ignore\n",
    "        # If any label is >= vocab_size, clamp it\n",
    "        if hasattr(self.tokenizer, 'vocab_size'):\n",
    "            vocab_sz = self.tokenizer.vocab_size\n",
    "            labels = torch.clamp(labels, min=-100, max=vocab_sz - 1)\n",
    "        return { 'input_ids': input_ids, 'labels': labels }\n",
    "\n",
    "# Create data loaders - supports either a list of texts or a dataset name/file-based StreamingTextDataset\n",
    "def create_dataloaders(texts_or_name, tokenizer, batch_size, seq_len=2048):\n",
    "    # If a Python list (in-memory texts), use InMemoryTextDataset\n",
    "    if isinstance(texts_or_name, list):\n",
    "        dataset = InMemoryTextDataset(texts_or_name, tokenizer, seq_len=seq_len)\n",
    "    else:\n",
    "        # assume a dataset name string for StreamingTextDataset\n",
    "        dataset = StreamingTextDataset(dataset_name=texts_or_name, seq_len=seq_len, tokenizer=tokenizer)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        pin_memory=False  # Disable pin_memory to avoid CUDA assert during memory pinning; we'll move to device manually in training_step\n",
    "    )\n",
    "\n",
    "# Initialize training\n",
    "model_pl = CRSMLightningModule(model, tokenizer)\n",
    "# Use seq_len from config\n",
    "seq_len = getattr(config, 'max_position_embeddings', 2048)\n",
    "\n",
    "# If base_data entries are lists of strings, pass that; else pass dataset name\n",
    "train_loader = create_dataloaders(base_data['train'], tokenizer, training_params['batch_size'], seq_len=seq_len)\n",
    "val_loader = create_dataloaders(base_data['validation'], tokenizer, training_params['batch_size'], seq_len=seq_len)\n",
    "\n",
    "# Set up trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='cuda',\n",
    "    devices=1,\n",
    "    max_epochs=10,  # Adjust as needed\n",
    "    precision='16-mixed',\n",
    "    accumulate_grad_batches=training_params['gradient_accum'],\n",
    "    default_root_dir=LOGS_DIR,\n",
    "    enable_checkpointing=True,\n",
    "    val_check_interval=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb51c7b",
   "metadata": {},
   "source": [
    "# 7. Train Base Model\n",
    "\n",
    "Let's train our base model and monitor the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd71d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the base model\n",
    "trainer.fit(model_pl, train_loader, val_loader)\n",
    "\n",
    "# Save the trained model\n",
    "model_pl.model.save_pretrained(f\"{CHECKPOINTS_DIR}/base_model\")\n",
    "tokenizer.save_pretrained(f\"{CHECKPOINTS_DIR}/base_model\")\n",
    "\n",
    "print(\"Base model training completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284430f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging cell: single-batch smoke test without pin_memory issues\n",
    "# Directly inspect dataset __getitem__ before the dataloader tries to pin it to GPU\n",
    "import os, traceback, torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running smoke test on device:', device)\n",
    "\n",
    "# Check tokenizer vocab_size first\n",
    "vocab_size = getattr(tokenizer, 'vocab_size', None)\n",
    "print('tokenizer.vocab_size:', vocab_size)\n",
    "\n",
    "# Get the underlying dataset (before DataLoader wrapping)\n",
    "# train_loader.dataset gives us the InMemoryTextDataset or StreamingTextDataset\n",
    "underlying_dataset = train_loader.dataset\n",
    "print('underlying_dataset type:', type(underlying_dataset).__name__)\n",
    "\n",
    "# Get one raw sample from the dataset (bypasses DataLoader/pin_memory)\n",
    "try:\n",
    "    raw_sample = underlying_dataset[0]\n",
    "    print('raw sample keys:', list(raw_sample.keys()) if isinstance(raw_sample, dict) else type(raw_sample))\n",
    "    print('input_ids shape/dtype:', raw_sample['input_ids'].shape, raw_sample['input_ids'].dtype)\n",
    "    print('labels shape/dtype:', raw_sample['labels'].shape, raw_sample['labels'].dtype)\n",
    "    \n",
    "    # Check label ranges\n",
    "    lbl_min = int(raw_sample['labels'].min().item())\n",
    "    lbl_max = int(raw_sample['labels'].max().item())\n",
    "    print('labels min/max:', lbl_min, lbl_max)\n",
    "    if vocab_size is not None and lbl_max >= vocab_size:\n",
    "        print(f'>>> ERROR: label value {lbl_max} >= tokenizer.vocab_size {vocab_size}')\n",
    "    if lbl_min < -100:\n",
    "        print(f'>>> WARNING: label value {lbl_min} < -100 (unusual; typically -100 is ignore_index)')\n",
    "except Exception as e:\n",
    "    print('Failed to get raw sample from dataset:')\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Manually collate a batch (simulating what DataLoader does, but without pin_memory)\n",
    "try:\n",
    "    print('\\nCollating batch manually (no pin_memory)...')\n",
    "    batch_samples = [underlying_dataset[i] for i in range(min(4, len(underlying_dataset)))]\n",
    "    \n",
    "    # Stack input_ids and labels\n",
    "    batch = {\n",
    "        'input_ids': torch.stack([s['input_ids'] for s in batch_samples]),\n",
    "        'labels': torch.stack([s['labels'] for s in batch_samples])\n",
    "    }\n",
    "    \n",
    "    print('Batch (on CPU):', {k: (v.dtype, v.device, v.shape) for k, v in batch.items()})\n",
    "    \n",
    "    # Move to device manually (avoids pin_memory CUDA assert)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print('Batch (on device):', {k: (v.dtype, v.device, v.shape) for k, v in batch.items()})\n",
    "    \n",
    "    input_ids = batch['input_ids']\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    # Run model forward\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids)\n",
    "        if isinstance(out, tuple) or isinstance(out, list):\n",
    "            logits = out[0]\n",
    "        else:\n",
    "            logits = out\n",
    "        print('logits shape/dtype/device:', logits.shape, logits.dtype, logits.device)\n",
    "        if vocab_size is not None and logits.shape[-1] != vocab_size:\n",
    "            print(f'>>> MISMATCH: logits last-dim={logits.shape[-1]} != tokenizer.vocab_size={vocab_size}')\n",
    "    \n",
    "    # Compute loss on CPU to avoid CUDA kernel asserts\n",
    "    llogits = logits.cpu()\n",
    "    llabels = labels.cpu()\n",
    "    shift_logits = llogits[:, :-1, :].reshape(-1, llogits.size(-1))\n",
    "    shift_labels = llabels.view(-1)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "    print('CrossEntropyLoss (computed on CPU):', float(loss.item()))\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Error during batch collation/forward/loss:')\n",
    "    traceback.print_exc()\n",
    "\n",
    "print('\\nSmoke test complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1fec9",
   "metadata": {},
   "source": [
    "# 8. Instruction Fine-tuning\n",
    "\n",
    "Now we'll fine-tune the base model on our instruction dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=2048):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # Format: Instruction \\n Input: {input} \\n Output: {output}\n",
    "        text = f\"{item['instruction']}\\nInput: {item['input']}\\nOutput: {item['output']}\"\n",
    "        \n",
    "        ids = self.tokenizer.encode(text)\n",
    "        # truncate/pad\n",
    "        if len(ids) > self.max_length:\n",
    "            ids = ids[:self.max_length]\n",
    "        else:\n",
    "            ids = ids + [self.tokenizer._simple.stoi.get('<pad>', 0)] * (self.max_length - len(ids))\n",
    "        input_ids = torch.tensor(ids, dtype=torch.long)\n",
    "        # labels are next-token (shifted)\n",
    "        labels = input_ids.clone()\n",
    "        # For causal LM training, typically label padding tokens are set to -100 to ignore\n",
    "        labels[labels == self.tokenizer._simple.stoi.get('<pad>', 0)] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": (input_ids != self.tokenizer._simple.stoi.get('<pad>', 0)).long(),\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72eff75",
   "metadata": {},
   "source": [
    "# 9. Model Validation and Testing\n",
    "\n",
    "Let's validate our models on some test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "import evaluate\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test base model on text completion\n",
    "base_model = CRSMModel.from_pretrained(f\"{CHECKPOINTS_DIR}/base_model\")\n",
    "base_model.eval()\n",
    "base_model.cuda()\n",
    "\n",
    "print(\"Base model completion test:\")\n",
    "prompt = \"The theory of relativity states that\"\n",
    "completion = generate_text(base_model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Completion: {completion}\\n\")\n",
    "\n",
    "# Test instruction model on various tasks\n",
    "instruct_model = CRSMModel.from_pretrained(f\"{CHECKPOINTS_DIR}/instruct_model\")\n",
    "instruct_model.eval()\n",
    "instruct_model.cuda()\n",
    "\n",
    "test_instructions = [\n",
    "    {\n",
    "        \"instruction\": \"Explain what photosynthesis is in simple terms.\",\n",
    "        \"input\": \"\",\n",
    "        \"reference\": \"Photosynthesis is how plants make their food using sunlight, water, and carbon dioxide.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a haiku about autumn.\",\n",
    "        \"input\": \"\",\n",
    "        \"reference\": \"Leaves drift to the ground\\nCool breeze whispers through branches\\nAutumn paints the world\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "print(\"Instruction model tests:\")\n",
    "for test in test_instructions:\n",
    "    prompt = f\"{test['instruction']}\\nInput: {test['input']}\\nOutput:\"\n",
    "    response = generate_text(instruct_model, tokenizer, prompt)\n",
    "    \n",
    "    print(f\"\\nInstruction: {test['instruction']}\")\n",
    "    print(f\"Model response: {response}\")\n",
    "    print(f\"Reference: {test['reference']}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rouge_scores = rouge.compute(predictions=[response], references=[test['reference']])\n",
    "    bleu_score = bleu.compute(predictions=[response.split()], references=[[test['reference'].split()]])\n",
    "    \n",
    "    print(f\"ROUGE-L: {rouge_scores['rougeL']:.3f}\")\n",
    "    print(f\"BLEU: {bleu_score['bleu']:.3f}\")\n",
    "\n",
    "print(\"\\nValidation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0964258",
   "metadata": {},
   "source": [
    "# 10. Save and Export Models\n",
    "\n",
    "Finally, let's save our models in a format ready for deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model card information\n",
    "model_card = f\"\"\"\n",
    "# CRSM 2B Model\n",
    "\n",
    "This model was trained as a proof-of-concept implementation of the CRSM architecture.\n",
    "\n",
    "## Model Details\n",
    "- Parameters: ~2B\n",
    "- Architecture: CRSM with {config.num_hidden_layers} layers\n",
    "- Context Length: {config.max_position_embeddings} tokens\n",
    "- Vocab Size: {config.vocab_size} tokens\n",
    "\n",
    "## Training\n",
    "- Base training: RedPajama dataset subset\n",
    "- Instruction tuning: Dolly dataset\n",
    "- Training platform: Google Colab\n",
    "- Hardware: {nvmlDeviceGetName(nvmlDeviceGetHandleByIndex(0)).decode('utf-8')}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from crsm.model import CRSMModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = CRSMModel.from_pretrained(\"path_to_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path_to_model\")\n",
    "\n",
    "# Generate text\n",
    "text = generate_text(model, tokenizer, \"Your prompt here\")\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "- Proof-of-concept implementation\n",
    "- Limited training data\n",
    "- Colab resource constraints\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{CHECKPOINTS_DIR}/base_model/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "with open(f\"{CHECKPOINTS_DIR}/instruct_model/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Optional: Push to Hugging Face Hub\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# \n",
    "# instruct_model.push_to_hub(\"your-username/crsm-2b-instruct\")\n",
    "# tokenizer.push_to_hub(\"your-username/crsm-2b-instruct\")\n",
    "\n",
    "print(\"Models saved and exported successfully!\")\n",
    "print(f\"You can find the models in your Google Drive at: {CHECKPOINTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
