{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16555dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook repaired cell placeholder\n",
    "# This file is managed by the repo. If you edit it here, ensure compatibility with the installed environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b176a9",
   "metadata": {},
   "source": [
    "# CRSM Colab Runbook\n",
    "\n",
    "This notebook scaffolds running distillation or training for the CRSM project on Google Colab. It mounts Google Drive, installs dependencies (preferring `mamba` when available), and provides interactive cells to choose whether to run distillation or training, configure hyperparameters, and save checkpoints to Drive.\n",
    "\n",
    "Use the cells in order. The notebook supports using Gemini 2.5 Flash as the default teacher but allows switching to other providers or a local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fdf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Detect environment and prefer mamba for package installation\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "HAS_MAMBA = shutil.which('mamba') is not None\n",
    "print('mamba installed:', HAS_MAMBA)\n",
    "\n",
    "# Provide helper install command variable\n",
    "INSTALL_CMD = 'mamba install -y --skip-existing -c conda-forge' if HAS_MAMBA else 'pip install'\n",
    "print('Using install command:', INSTALL_CMD)\n",
    "\n",
    "# CUDA check\n",
    "import torch\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('torch version:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install core dependencies (uses INSTALL_CMD from previous cell)\n",
    "print('Installing CRSM dependencies...')\n",
    "cmd = f\"{INSTALL_CMD} torch einops transformers datasets tokenizers wandb\"\n",
    "print('Running:', cmd)\n",
    "import subprocess\n",
    "res = subprocess.run(cmd, shell=True)\n",
    "print('Install exit code:', res.returncode)\n",
    "print('If you used pip, you may need to restart the runtime before importing heavy packages like torch or transformers.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236416ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Mount Google Drive (optional) and set WORKDIR\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "WORKDIR = '/content/drive/MyDrive/crsm_runs'\n",
    "import os\n",
    "os.makedirs(WORKDIR, exist_ok=True)\n",
    "print('WORKDIR:', WORKDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacec9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Example - create HF tokenizer and small streaming dataset\n",
    "from crsm.tokenizer import Tokenizer\n",
    "from crsm.dataset import StreamingTextDataset\n",
    "\n",
    "# Example: use a small HF tokenizer name if available, else use fallback\n",
    "try:\n",
    "    tok = Tokenizer(hf_name='sshleifer/tiny-gpt2')\n",
    "    print('Loaded HF tokenizer, vocab_size=', tok.vocab_size)\n",
    "except Exception as e:\n",
    "    print('HF tokenizer not available, using fallback:', e)\n",
    "    tok = Tokenizer()\n",
    "\n",
    "# Create a streaming dataset from WORKDIR/texts (create that folder and upload files there)\n",
    "ds = StreamingTextDataset(data_dir=WORKDIR + '/texts', seq_len=64, hf_tokenizer_name=None)\n",
    "print('Streaming dataset created; first few examples:')\n",
    "for i, (inp, tgt) in enumerate(ds):\n",
    "    print('inp.shape=', inp.shape, 'tgt.shape=', tgt.shape)\n",
    "    if i >= 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a254b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Example - run a short training job (small, for demo)\n",
    "# This runs the CLI train command in-process. For longer runs, use the CLI in a shell.\n",
    "\n",
    "from crsm.train import main as train_main\n",
    "\n",
    "train_main(epochs=1, batch_size=8, vocab_size=1000, seq_len=32, lr=1e-3, data_dir=None, checkpoint_dir=WORKDIR + '/checkpoints', wandb_enabled=False)\n",
    "print('Training demo finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Example - generate distillation traces from prompts file using distill_runner\n",
    "# Create a small prompts file and run the distill runner (this will attempt to call the provider)\n",
    "prompts_path = WORKDIR + '/prompts.txt'\n",
    "with open(prompts_path, 'w') as f:\n",
    "    f.write('Write a concise chain-of-thought explanation for why 2+2=4\\n')\n",
    "\n",
    "out_path = WORKDIR + '/distill_traces.jsonl'\n",
    "\n",
    "# run distill runner (note: requires provider libs/API keys to be configured)\n",
    "from crsm.distill_runner import generate_batch\n",
    "\n",
    "try:\n",
    "    generate_batch([\"Explain why gravity causes objects to fall.\"], provider_kind='local', out_path=out_path, prompt_erasure=False, workers=1)\n",
    "    print('Wrote traces to', out_path)\n",
    "except Exception as e:\n",
    "    print('Distillation generation failed (expected if provider libs not installed or API keys missing):', e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
