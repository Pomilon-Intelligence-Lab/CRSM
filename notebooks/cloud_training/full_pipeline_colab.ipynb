{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRSM: End-to-End Cloud Training Pipeline\n",
    "\n",
    "This notebook runs the complete 4-stage training pipeline for the Continuous Reasoning State Model (CRSM) on Google Colab.\n",
    "\n",
    "**Stages:**\n",
    "1.  **Data Preparation:** Download and tokenize FineWeb-Edu/GSM8K.\n",
    "2.  **Stage 1 (System 1):** Train the Mamba backbone.\n",
    "3.  **Stage 2 (Subconscious):** Distill the Latent Dynamics Model.\n",
    "4.  **Stage 3 (Judgment):** Train the Value Head via offline expert iteration.\n",
    "5.  **Stage 4 (Assembly):** Assemble the final artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Create experiment directories in Drive\n",
    "!mkdir -p /content/drive/MyDrive/crsm_experiments/stage_{1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Repository\n",
    "!git clone https://github.com/Pomilon-Intelligence-Lab/crsm.git\n",
    "%cd crsm\n",
    "\n",
    "# Install Dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "We prepare the **FineWeb-Edu** dataset for the backbone and **GSM8K** for reasoning tasks.\n",
    "The data is tokenized and saved as `uint16` binary files for memory-mapped streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare FineWeb-Edu (Sample)\n",
    "# Note: This might take a while. We use a small shard size to see results quickly.\n",
    "!python scripts/data/prepare_dataset.py --dataset fineweb --subset sample-10BT --shard-size 20000000 --output-dir data/fineweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GSM8K (Reasoning)\n",
    "!python scripts/data/prepare_dataset.py --dataset gsm8k --output-dir data/gsm8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 1: Backbone Training (System 1)\n",
    "We train the Mamba backbone on the prepared FineWeb data.\n",
    "We use the `baseline_27m` configuration but override the data directory and set epoch count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/training/stage_1_backbone.py \\",
    "    --config configs/baseline_27m.yaml \\",
    "    --data-dir data/fineweb \\",
    "    --epochs 1 \\",
    "    --no-wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup to Drive\n",
    "!cp experiments/stage_1/backbone_final.pt /content/drive/MyDrive/crsm_experiments/stage_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 2: Dynamics Distillation (The Subconscious)\n",
    "We freeze the backbone and train the latent dynamics model to predict state transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/training/stage_2_dynamics.py \\",
    "    --config configs/baseline_27m.yaml \\",
    "    --epochs 2 \\",
    "    --samples 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup to Drive\n",
    "!cp experiments/stage_2/dynamics_final.pt /content/drive/MyDrive/crsm_experiments/stage_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 3: Value Head Training (The Judgment)\n",
    "We train the Value Head using offline MCTS rollouts to recognize high-quality reasoning paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/training/stage_3_value_head.py \\",
    "    --config configs/baseline_27m.yaml \\",
    "    --epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup to Drive\n",
    "!cp experiments/stage_3/backbone_with_value.pt /content/drive/MyDrive/crsm_experiments/stage_3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 4: Assembly & Verification\n",
    "We combine the trained components into the final CRSM artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/training/stage_4_assembly.py \\",
    "    --config configs/baseline_27m.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model to Drive\n",
    "!cp experiments/stage_4/crsm_final.pt /content/drive/MyDrive/crsm_experiments/stage_4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Demo\n",
    "Load the assembled model and run a test generation with the \"Thinking\" loop active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import asyncio\n",
    "from crsm.model import CRSMModel, CRSMConfig\n",
    "from crsm.tokenizer import Tokenizer\n",
    "\n",
    "# Load Model\n",
    "checkpoint_path = \"experiments/stage_4/crsm_final.pt\"\n",
    "ckpt = torch.load(checkpoint_path)\n",
    "config = CRSMConfig.from_dict(ckpt['config']['model'])\n",
    "config.autonomous_mode = True # Enable background thinking\n",
    "\n",
    "model = CRSMModel(config).cuda()\n",
    "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "model.load_dynamics(\"experiments/stage_2/dynamics_final.pt\") # Ensure dynamics are loaded\n",
    "model.eval()\n",
    "\n",
    "tokenizer = Tokenizer(\"gpt2\")\n",
    "\n",
    "async def generate_demo(text):\n",
    "    print(f\"Prompt: {text}\")\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)]).cuda()\n",
    "    \n",
    "    output_ids = await model.crsm.think_and_generate(\n",
    "        input_ids, \n",
    "        max_length=50, \n",
    "        use_deliberation=True, \n",
    "        deliberation_lag=3\n",
    "    )\n",
    "    \n",
    "    output_text = tokenizer.decode(output_ids.tolist())\n",
    "    print(f\\\"\\nGenerated: {output_text}\\\")\n",
    "\n",
    "# Run\n",
    "await generate_demo(\"The future of artificial intelligence depends on\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
