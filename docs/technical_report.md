# CRSM: Technical Retrospective
**Subtitle:** Engineering a "Subconscious" for State Space Models

> **Transparency Note:** This project is a "Centaur" collaboration. The core idea and initial architectural research were generated by **Gemini 2.5 Flash** in response to the author's prompt for a "continuous reasoning" model. The implementation, testing, and critical problem-solving (like the Gated Injection fix) were human-led.

## 1. The Motivation
The primary limitation of current Large Language Models is their reliance on token generation for reasoning. To "think" about a problem, a Transformer must output a sequence of tokens (Chain of Thought). This introduces significant latency and entangles the reasoning process with the generation process.

The goal of **CRSM (Continuous Reasoning State Model)** was to explore a distinct alternative: **Can we attach a background process—a "subconscious"—that performs lookahead planning while the model generates text?**

This architecture attempts to decouple the "System 1" (fast, intuitive generation) from the "System 2" (slow, deliberate planning) in a way that mimics biological efficiency.

## 2. The Architecture

### The Backbone: Mamba
I selected **Mamba** (a State Space Model) rather than a Transformer for the "System 1" backbone. The reasoning was twofold:
1.  **Efficiency:** Mamba's linear-time complexity fits the goal of low-latency generation.
2.  **State Manipulability:** Unlike the growing Key-Value cache of a Transformer, Mamba maintains a fixed-size compressed latent state `h`. This presents a distinct target for direct modification: *if we can adjust `h`, we can potentially guide the model's reasoning trajectory without altering the token sequence.*

### The Planner: Asynchronous MCTS
An **Asynchronous Monte Carlo Tree Search (MCTS)** engine runs in a parallel thread. As the Mamba backbone generates tokens `t, t+1`, the planner takes a snapshot of the state at `t`. It performs multiple rollouts to evaluate potential future states.

To make this computationally feasible, I trained a distilled **Latent Dynamics Model**. Initially a simple MLP, this was upgraded to a **GRUCell-based Recurrent Model**. This "world model" can now capture temporal dependencies and state residuals ($h_t \to h_{t+1}$), allowing for high-fidelity simulations during MCTS rollouts without invoking the heavy backbone.

### The Policy: Hierarchical Fusion
To ensure the model makes decisions based on all levels of abstraction, CRSM uses **Hierarchical Policy Fusion**. Instead of predicting tokens only from the final layer, we implement a **Learned Weighted Sum** of all layer states. This allows the model to balance high-level strategic "rules" with raw spatial and syntactic context.

## 3. The State Integration Challenge

The most significant engineering hurdle was integrating the planner's output back into the backbone.

My initial approach was naive: simply adding the planner's "target state delta" to the current model state (`state += delta`).
**Result:** Immediate destabilization. The Mamba state space is a sensitive manifold; additive noise destroyed the coherence of the context history, leading to garbage output.

### The Solution: Sparse-Gated Hierarchical Injection
The final solution was to implement **Sparse-Gated Hierarchical Injection**. Each layer in the Mamba hierarchy is treated as a sovereign entity with its own independent gate.

Instead of a single global update, the system uses a learned interpolation for each layer $i$:
$$h_{i,new} = (1 - \alpha_i) \cdot h_{i,old} + \alpha_i \cdot h_{i,target}$$

This acts effectively as a low-pass filter for reasoning signals. The planner uses a **Multi-Headed Value Critic (MV-Critic)** to determine layer-wise confidence. It can now aggressively "nudge" high-level strategy layers towards a higher-value state while leaving low-level syntax layers untouched. This preserves the manifold stability of the original backbone while still allowing for corrective guidance.

Additionally, we introduced a **Targeted Delta Buffer** to ensure precise **State-Step Alignment**. Plans optimized for a future token position are held in the buffer and injected exactly when the generation loop reaches that specific step, solving the mathematical misalignment caused by asynchronous latency.

## 4. Engineering The Training Pipeline

To enable scalable research and rapid iteration on reasoning tasks, we refactored the codebase into a modular, task-agnostic system.

### The Engine: Trainer
The procedural training scripts were unified into a generic `Trainer` class. This engine handles the "How" of training: gradient accumulation, mixed-precision (AMP), and checkpointing. It is entirely unaware of whether it is training a language model or a spatial reasoning module.

### The Logic: Tasks
All domain-specific knowledge is encapsulated in **Tasks** (`crsm/tasks/`).
1.  **`LanguageModelingTask`**: Standard CLM training with **Multi-Headed Value Critic** supervision and **Hierarchical Entropy Loss** to ensure multi-layer feature fusion.
2.  **`DistillationTask`**: Stage 2 pipeline where the Broadcaster learns state residuals from collected traces.
3.  **`ARCTask`**: (In Development) Specialized logic for grid-based spatial reasoning on the ARC-AGI benchmark.

This decoupling allows us to target **Nano-scale models (100k - 500k parameters)** by fine-tuning reasoning strategies on extremely small but efficient backbones.

**Data Efficiency:**
To handle large-scale corpora, the `data/` domain provides streaming datasets using memory mapping (`numpy.memmap`). This minimizes RAM usage and tokenization overhead, allowing research to proceed on standard consumer hardware.

## 5. Current Status
*   **Infrastructure:** The full 4-stage training pipeline is implemented and verified on synthetic data.
*   **Hierarchical Stability:** Verified via stress tests (1000+ hierarchical injections). The Sparse-Gated mechanism successfully maintains numerical stability across all abstraction levels.
*   **World Model Fidelity:** The recurrent dynamics model achieves **0.99+ Cosine Similarity** in state transition prediction, enabling deep lookahead.
*   **Precise Alignment:** The Targeted Delta Buffer successfully resolves asynchronous drift, ensuring plans are applied at the exact step they were optimized for.
*   **Validation:** All 30 core tests and 4 specialized verification scripts pass. The system is now mathematically valid and benchmarking on ARC-AGI tasks has commenced.
*   **ARC-AGI Early Results:** Nano-scale models (118k params) have successfully demonstrated the ability to learn grid syntax and local rule application (identity) with a 75%+ loss reduction in initial training runs. Multi-step global reasoning evaluation is ongoing.

## 6. Inspirations & Parallels
This architecture draws heavily from existing research:
*   **Representation Engineering (RepE):** The idea of steering generation via latent space manipulation.
*   **MuZero & AlphaLLM:** The application of MCTS to latent states and language models.
*   **Polyak Averaging:** The mathematical basis for the stable "soft update" injection.

## 7. Current Constraints & Challenges
While the architecture functions, several practical bottlenecks remain:

1.  **Global Interpreter Lock (GIL):** The implementation uses Python's `asyncio`, which is not truly parallel due to the GIL. This causes minor stuttering in generation when the planner is under heavy load. A robust implementation would require the planner to be implemented in C++ or Rust.
2.  **Memory Overhead:** Storing the MCTS tree states requires significant VRAM. This has been optimized by re-computing states on the fly (trading compute for memory), but it limits the search depth on consumer hardware.
3.  **The "Bootstrapping" Problem:** The planner relies on a trained Value Head to identify "good" states. However, the Value Head requires data from the planner to learn. This circular dependency makes the initial training phase slow and unstable.

## 8. Conclusion
CRSM remains an experimental prototype. It has successfully demonstrated that **state injection in SSMs is mathematically viable** and that **asynchronous planning is architecturally possible**. The next phase of research focuses on scaling the training pipeline to produce a planner capable of genuine strategic insight.
