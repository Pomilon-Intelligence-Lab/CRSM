# CRSM: Technical Retrospective
**Subtitle:** Engineering a "Subconscious" for State Space Models

> **Transparency Note:** This project is a "Centaur" collaboration. The core idea and initial architectural research were generated by **Gemini 2.5 Flash** in response to the author's prompt for a "continuous reasoning" model. The implementation, testing, and critical problem-solving (like the Gated Injection fix) were human-led.

## 1. The Motivation
The primary limitation of current Large Language Models is their reliance on token generation for reasoning. To "think" about a problem, a Transformer must output a sequence of tokens (Chain of Thought). This introduces significant latency and entangles the reasoning process with the generation process.

The goal of **CRSM (Continuous Reasoning State Model)** was to explore a distinct alternative: **Can we attach a background process—a "subconscious"—that performs lookahead planning while the model generates text?**

This architecture attempts to decouple the "System 1" (fast, intuitive generation) from the "System 2" (slow, deliberate planning) in a way that mimics biological efficiency.

## 2. The Architecture

### The Backbone: Mamba
I selected **Mamba** (a State Space Model) rather than a Transformer for the "System 1" backbone. The reasoning was twofold:
1.  **Efficiency:** Mamba's linear-time complexity fits the goal of low-latency generation.
2.  **State Manipulability:** Unlike the growing Key-Value cache of a Transformer, Mamba maintains a fixed-size compressed latent state `h`. This presents a distinct target for direct modification: *if we can adjust `h`, we can potentially guide the model's reasoning trajectory without altering the token sequence.*

### The Planner: Asynchronous MCTS
An **Asynchronous Monte Carlo Tree Search (MCTS)** engine runs in a parallel thread. As the Mamba backbone generates tokens `t, t+1`, the planner takes a snapshot of the state at `t`. It performs multiple rollouts to evaluate potential future states.

To make this computationally feasible, I trained a distilled **Latent Dynamics Model**—a lightweight Multi-Layer Perceptron (MLP) that approximates the state transitions of the full backbone. This allows the planner to act as a rapid "world model" simulator.

## 3. The State Integration Challenge

The most significant engineering hurdle was integrating the planner's output back into the backbone.

My initial approach was naive: simply adding the planner's "target state delta" to the current model state (`state += delta`).
**Result:** Immediate destabilization. The Mamba state space is a sensitive manifold; additive noise destroyed the coherence of the context history, leading to garbage output.

### The Solution: Gated State Injection
The fix was to implement a **Gated Injection** mechanism. Instead of addition, the system uses a learned interpolation:
$$h_{new} = (1 - \alpha) \cdot h_{old} + \alpha \cdot h_{target}$$

This acts effectively as a low-pass filter for reasoning signals. The planner gently "nudges" the model's intuition towards a higher-value state. This preserves the manifold stability of the original backbone while still allowing for corrective guidance.

## 4. Current Status
*   **Stability:** Verified via stress tests (1000+ continuous injections). The Gated Injection mechanism successfully maintains numerical stability.
*   **Safety:** A "Confidence Scaling" mechanism ensures that if the planner's Value Head is uncertain, the injection rate ($\alpha$) drops to zero, preventing the planner from degrading the backbone's performance.
*   **Validation:** Preliminary tests verify that *if* the planner identifies a state with lower expected loss, the injection mechanism successfully steers the generation towards it.

## 5. Current Constraints & Challenges
While the architecture functions, several practical bottlenecks remain:

1.  **Global Interpreter Lock (GIL):** The implementation uses Python's `asyncio`, which is not truly parallel due to the GIL. This causes minor stuttering in generation when the planner is under heavy load. A robust implementation would require the planner to be implemented in C++ or Rust.
2.  **Memory Overhead:** Storing the MCTS tree states requires significant VRAM. This has been optimized by re-computing states on the fly (trading compute for memory), but it limits the search depth on consumer hardware.
3.  **The "Bootstrapping" Problem:** The planner relies on a trained Value Head to identify "good" states. However, the Value Head requires data from the planner to learn. This circular dependency makes the initial training phase slow and unstable.

## 6. Conclusion
CRSM remains an experimental prototype. It has successfully demonstrated that **state injection in SSMs is mathematically viable** and that **asynchronous planning is architecturally possible**. The next phase of research focuses on scaling the training pipeline to produce a planner capable of genuine strategic insight.
